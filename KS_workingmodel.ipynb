{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "import gensim.models.doc2vec as doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data & clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>name</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bcy7ky</td>\n",
       "      <td>15 minutes wait to play more?</td>\n",
       "      <td>So I dodged a game, all my mains banned or pic...</td>\n",
       "      <td>2rfxx</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>15 minutes wait to play more? So I dodged a ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bcy6mf</td>\n",
       "      <td>League has to investigate possible fake game b...</td>\n",
       "      <td>Obvious inting in game 5 right before TSM win ...</td>\n",
       "      <td>2rfxx</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>League has to investigate possible fake game b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>bcz8v5</td>\n",
       "      <td>Xmithie...the Goat, I told you so</td>\n",
       "      <td>Here's what I said about Xmithie years ago...\\...</td>\n",
       "      <td>2rfxx</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>Xmithie...the Goat, I told you so Here's what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>bcz9r2</td>\n",
       "      <td>Whatever happened to that Teacher in Korea who...</td>\n",
       "      <td>Anyone remember this ? It was way back like 20...</td>\n",
       "      <td>2rfxx</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>Whatever happened to that Teacher in Korea who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>bcz9wj</td>\n",
       "      <td>Just got to diamond playing annie only :)</td>\n",
       "      <td>Just wanted to share my excitement with you gu...</td>\n",
       "      <td>2rfxx</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>Just got to diamond playing annie only :) Just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>bcza0d</td>\n",
       "      <td>Hot Take: TSM vs TL is the Best Finals in LCS ...</td>\n",
       "      <td>As a TSM fan it sucks to lose, but those games...</td>\n",
       "      <td>2rfxx</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>Hot Take: TSM vs TL is the Best Finals in LCS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>bczb3x</td>\n",
       "      <td>Doublelift never meet Faker before in an offic...</td>\n",
       "      <td>As the title, I just realized somehow Doubleli...</td>\n",
       "      <td>2rfxx</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>Doublelift never meet Faker before in an offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>bczbqf</td>\n",
       "      <td>Griffin went balls to the walls to make LS loo...</td>\n",
       "      <td>after the whole LS Vs Reddit vs other casters ...</td>\n",
       "      <td>2rfxx</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>Griffin went balls to the walls to make LS loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>bczbu3</td>\n",
       "      <td>Dodged ranked game due to client bug :)</td>\n",
       "      <td>&amp;#x200B;\\r\\r\\n\\r\\r\\nhttps://i.redd.it/fj8xph83...</td>\n",
       "      <td>2rfxx</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>Dodged ranked game due to client bug :) &amp;#x200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>bczdhn</td>\n",
       "      <td>Decided to re-roll the skin shards I had saved...</td>\n",
       "      <td>https://imgur.com/Ezbocp6\\r\\r\\n\\r\\r\\nhttps://i...</td>\n",
       "      <td>2rfxx</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>Decided to re-roll the skin shards I had saved...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 submission_id  \\\n",
       "0           0        bcy7ky   \n",
       "1           1        bcy6mf   \n",
       "2           2        bcz8v5   \n",
       "3           3        bcz9r2   \n",
       "4           4        bcz9wj   \n",
       "5           5        bcza0d   \n",
       "6           6        bczb3x   \n",
       "7           7        bczbqf   \n",
       "8           8        bczbu3   \n",
       "9           9        bczdhn   \n",
       "\n",
       "                                               title  \\\n",
       "0                      15 minutes wait to play more?   \n",
       "1  League has to investigate possible fake game b...   \n",
       "2                  Xmithie...the Goat, I told you so   \n",
       "3  Whatever happened to that Teacher in Korea who...   \n",
       "4          Just got to diamond playing annie only :)   \n",
       "5  Hot Take: TSM vs TL is the Best Finals in LCS ...   \n",
       "6  Doublelift never meet Faker before in an offic...   \n",
       "7  Griffin went balls to the walls to make LS loo...   \n",
       "8            Dodged ranked game due to client bug :)   \n",
       "9  Decided to re-roll the skin shards I had saved...   \n",
       "\n",
       "                                                text subreddit_id  \\\n",
       "0  So I dodged a game, all my mains banned or pic...        2rfxx   \n",
       "1  Obvious inting in game 5 right before TSM win ...        2rfxx   \n",
       "2  Here's what I said about Xmithie years ago...\\...        2rfxx   \n",
       "3  Anyone remember this ? It was way back like 20...        2rfxx   \n",
       "4  Just wanted to share my excitement with you gu...        2rfxx   \n",
       "5  As a TSM fan it sucks to lose, but those games...        2rfxx   \n",
       "6  As the title, I just realized somehow Doubleli...        2rfxx   \n",
       "7  after the whole LS Vs Reddit vs other casters ...        2rfxx   \n",
       "8  &#x200B;\\r\\r\\n\\r\\r\\nhttps://i.redd.it/fj8xph83...        2rfxx   \n",
       "9  https://imgur.com/Ezbocp6\\r\\r\\n\\r\\r\\nhttps://i...        2rfxx   \n",
       "\n",
       "              name                                           all_text  \n",
       "0  leagueoflegends  15 minutes wait to play more? So I dodged a ga...  \n",
       "1  leagueoflegends  League has to investigate possible fake game b...  \n",
       "2  leagueoflegends  Xmithie...the Goat, I told you so Here's what ...  \n",
       "3  leagueoflegends  Whatever happened to that Teacher in Korea who...  \n",
       "4  leagueoflegends  Just got to diamond playing annie only :) Just...  \n",
       "5  leagueoflegends  Hot Take: TSM vs TL is the Best Finals in LCS ...  \n",
       "6  leagueoflegends  Doublelift never meet Faker before in an offic...  \n",
       "7  leagueoflegends  Griffin went balls to the walls to make LS loo...  \n",
       "8  leagueoflegends  Dodged ranked game due to client bug :) &#x200...  \n",
       "9  leagueoflegends  Decided to re-roll the skin shards I had saved...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/ksmith/Documents/Code/DS1/Unit4_Project/build-post-here-DS/plz_work.csv')\n",
    "df = df.fillna('')\n",
    "df['all_text'] = df['title'] + ' ' + df['text'] # Combining text & titles to one field\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>name</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>bcy7k8</td>\n",
       "      <td>[Spoilers] What is that one conspiracy theory ...</td>\n",
       "      <td>Arya is already dead and is actually Jaqen H'g...</td>\n",
       "      <td>2rjz2</td>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>[Spoilers] What is that one conspiracy theory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>bcz9cw</td>\n",
       "      <td>[SPOILERS] The only reunion that I want to see...</td>\n",
       "      <td></td>\n",
       "      <td>2rjz2</td>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>[SPOILERS] The only reunion that I want to see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>bczaow</td>\n",
       "      <td>[No Spoilers] How does rising the dead work?</td>\n",
       "      <td>Some stuff I wonder about.\\r\\r\\n\\r\\r\\nHow fres...</td>\n",
       "      <td>2rjz2</td>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>[No Spoilers] How does rising the dead work? S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>bczgts</td>\n",
       "      <td>[No Spoilers] House Targaryen sigil cross stitch</td>\n",
       "      <td></td>\n",
       "      <td>2rjz2</td>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>[No Spoilers] House Targaryen sigil cross stitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>bczhln</td>\n",
       "      <td>[No Spoilers] Need help!!</td>\n",
       "      <td>My exams(very important) are going on and will...</td>\n",
       "      <td>2rjz2</td>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>[No Spoilers] Need help!! My exams(very import...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    submission_id                                              title  \\\n",
       "551        bcy7k8  [Spoilers] What is that one conspiracy theory ...   \n",
       "552        bcz9cw  [SPOILERS] The only reunion that I want to see...   \n",
       "553        bczaow       [No Spoilers] How does rising the dead work?   \n",
       "554        bczgts   [No Spoilers] House Targaryen sigil cross stitch   \n",
       "555        bczhln                          [No Spoilers] Need help!!   \n",
       "\n",
       "                                                  text subreddit_id  \\\n",
       "551  Arya is already dead and is actually Jaqen H'g...        2rjz2   \n",
       "552                                                           2rjz2   \n",
       "553  Some stuff I wonder about.\\r\\r\\n\\r\\r\\nHow fres...        2rjz2   \n",
       "554                                                           2rjz2   \n",
       "555  My exams(very important) are going on and will...        2rjz2   \n",
       "\n",
       "              name                                           all_text  \n",
       "551  gameofthrones  [Spoilers] What is that one conspiracy theory ...  \n",
       "552  gameofthrones  [SPOILERS] The only reunion that I want to see...  \n",
       "553  gameofthrones  [No Spoilers] How does rising the dead work? S...  \n",
       "554  gameofthrones  [No Spoilers] House Targaryen sigil cross stitch   \n",
       "555  gameofthrones  [No Spoilers] Need help!! My exams(very import...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20 = ['AskReddit', 'dankmemes', 'memes', 'teenagers', 'aww', 'RocketLeagueExchange', 'Showerthoughts',\n",
    "          'funny', 'me_irl', 'freefolk', 'gameofthrones', 'pics', 'NoStupidQuestions', 'AskOuija',\n",
    "          'unpopularopinion', 'gaming', 'videos', 'politics', 'AmItheAsshole', 'Jokes']\n",
    "\n",
    "data = df[df['name'].isin(top_20)]\n",
    "data = data.drop(data.columns[0], axis=1) # Drop old row index\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def url_replace(string):\n",
    "    return re.sub('http\\S+|www.\\S+', lambda match: urlparse(match.group()).hostname, string)\n",
    "\n",
    "data['all_text'] = data['all_text'].apply(url_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "\n",
    "data['all_text'] = data['all_text'].str.lower()  #Text is lowercase\n",
    "data['all_text'] = data['all_text'].str.replace('\\r','')\n",
    "data['all_text'] = data['all_text'].str.replace('\\n','')\n",
    "data['all_text'] = data['all_text'].str.replace('/',' ')\n",
    "data['all_text'] = data['all_text'].str.replace('  ',' ')\n",
    "data['all_text'] = data['all_text'].str.replace('www','')\n",
    "data['all_text'] = data['all_text'].str.replace('com',' ')\n",
    "data['all_text'] = data['all_text'].str.translate(table) #Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551        spoilers  what is that one conspiracy theory ...\n",
       "552        spoilers  the only reunion that i want to see...\n",
       "553        no spoilers  how does rising the dead work  s...\n",
       "554        no spoilers  house targaryen sigil cross stitch \n",
       "555        no spoilers  need help   my exams very import...\n",
       "556        no spoilers  bookmaker in australia is offeri...\n",
       "557        spoilers  game of thrones season 8    first d...\n",
       "558        spoilers  is the nk similarly vulnerable   i ...\n",
       "559        no spoilers  game of thrones season 8 will gi...\n",
       "560        no spoilers  are you going to watch the premi...\n",
       "561        no spoilers  any links to share on where to w...\n",
       "562        spoilers  el paso meteorologist includes west...\n",
       "563        no spoilers  got  hide that spoiler app   fru...\n",
       "564        no spoilers  question about the book series h...\n",
       "565        no spoilers  live stream of a huge invite onl...\n",
       "566        no spoilers  got baby shark mashup everything...\n",
       "567        spoilers  house frakenfurter  don t dream it ...\n",
       "568        spoilers  my final predictions part 1 ok so t...\n",
       "569        spoilers  theory for s8 dany be es night quee...\n",
       "570        spoilers  my final predictions part 2 as for ...\n",
       "571                           no spoilers  one more day    \n",
       "572        spoilers  been making myself some custom card...\n",
       "573        spoilers  jon does a lot of stupid stuff that...\n",
       "574        no spoilers  rise and shine  game of thrones ...\n",
       "575        no spoilers  question about how to watch new ...\n",
       "576        no spoilers  what will you do for the throne ...\n",
       "577        spoilers  top 10 most evil game of thrones ch...\n",
       "578        no spoilers  grabbed the mcfarlane toys viser...\n",
       "579        no spoilers  will we be able to watch the epi...\n",
       "580        no spoilers  game of thrones cake i made  che...\n",
       "                                ...                        \n",
       "321855    a question about the dragon bones under the re...\n",
       "321856    agree or disagree  hillary clinton is the stan...\n",
       "321857                  no spoilers  dragon bones  removed \n",
       "321858                                  spoilers   removed \n",
       "321859     spoiler  what got tv series moments brought t...\n",
       "321860     spoliers  was the hand of the king      removed \n",
       "321861     spoilers  what got moment brought tears to yo...\n",
       "321862    listen and give your feedback to melisandre s ...\n",
       "321863     spoilers  listen and give your feedback to me...\n",
       "321864    would you say ramsey or joffrey is worse   rem...\n",
       "321865                      the long walk back     removed \n",
       "321866    1 v 1 what 2 characters in the show would you ...\n",
       "321867    no spoilers a question thats been on my mind f...\n",
       "321868                               no spoilers   removed \n",
       "321869    when did you first start watching the show   r...\n",
       "321870    what is the meaning of jon arryn s last words ...\n",
       "321871    what is the meaning of jon arryn s last words ...\n",
       "321872           spoiler  theory about king arrys  removed \n",
       "321873     spoilers  what is the meaning of jon arryn s ...\n",
       "321874                                      wn nr  removed \n",
       "321875    what is your take on the ramsay and myranda dy...\n",
       "321876     looking for a game of thrones necklace  removed \n",
       "321877    which of these characters do you think will di...\n",
       "321878    which of the characters in the linked image do...\n",
       "321879            on the last episode of season 7  removed \n",
       "321880     spoilers  will the iron throne be destroyed  ...\n",
       "321881     main spoilers  they couldn t have picked an a...\n",
       "321882                 final season release date   removed \n",
       "321883                     season 9 release date   removed \n",
       "321884        no spoiler  jaime lannister drawing  removed \n",
       "Name: all_text, Length: 87990, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['all_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_encode = {'AskReddit':1, 'dankmemes':2, 'memes':3, 'teenagers':4, 'aww':5, 'RocketLeagueExchange':6, \n",
    "#             'Showerthoughts':7,'funny':8, 'me_irl':9, 'freefolk':10, 'gameofthrones':11, 'pics':12, \n",
    "#             'NoStupidQuestions':13, 'AskOuija':14, 'unpopularopinion':15, 'gaming':16, 'videos':17, \n",
    "#             'politics':18, 'AmItheAsshole':19, 'Jokes':20}\n",
    "\n",
    "X = data['all_text']\n",
    "y = data['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['what', 's', 'the', 'most', 'embarrassing', 'thing', 'you', 've', 'done', 'in', 'front', 'of', 'a', 'crush'], tags=['Train_0']),\n",
       " TaggedDocument(words=['no', 'spoilers', 'what', 'will', 'you', 'do', 'for', 'the', 'throne', 'you', 'can', 'see', 'my', 'illustration', 'of', 'daenerys', 'there', 'for', 'a', 'split', 'second', '0', '30', 'and', 'other', 'amazing', 'creations'], tags=['Train_1']),\n",
       " TaggedDocument(words=['have', '10x', 'bigger', 'balls'], tags=['Train_2']),\n",
       " TaggedDocument(words=['is', 'it', 'better', 'to', 'overeat', 'healthy', 'food', 'or', 'undereat', 'unhealthy', 'food', 'i', 'looked', 'it', 'up', 'but', 'didn’t', 'get', 'much', 'of', 'a', 'definitive', 'answer', 'i', 'know', 'overeating', 'isn’t', 'really', 'good', 'for', 'you', 'and', 'neither', 'is', 'undereating', 'so', 'i', 'really', 'don’t', 'know'], tags=['Train_3']),\n",
       " TaggedDocument(words=['daario', 'post', 'malone', 'naharis'], tags=['Train_4']),\n",
       " TaggedDocument(words=['what’s', 'the', 'best', 'way', 'to', 'help', 'someone', 'with', 'depression', 'what’s', 'the', 'best', 'way', 'to', 'help', 'one', 'of', 'my', 'friends', 'who', 'is', 'depressed', 'i’ve', 'never', 'encountered', 'having', 'to', 'help', 'anyone', 'in', 'this', 'situation', 'and', 'i', 'want', 'to', 'do', 'my', 'best', 'to', 'help', 'her', 'however', 'i', 'just', 'dk', 'what', 'i', 'can', 'do', 'to', 'help', 'at', 'times', 'i', 'just', 'don’t', 'know', 'what', 'to', 'do', 'or', 'say', 'that', 'will', 'make', 'it', 'better'], tags=['Train_5']),\n",
       " TaggedDocument(words=['gottem', 'ladies', 'and', 'gents'], tags=['Train_6']),\n",
       " TaggedDocument(words=['heart', 'of', 'darkroast', 'visit', 'a', 'planet', 'of', 'coffee', 'to', 'track', 'down', 'viceroy', 'kurtz'], tags=['Train_7']),\n",
       " TaggedDocument(words=['are', 'crabs', 'able', 'to', 'escape', 'a', 'box', 'made', 'out', 'of', 'cloth', 'i’ve', 'had', 'a', 'dream', 'last', 'night', 'where', 'i', 'was', 'going', 'down', 'the', 'street', 'and', 'i', 'found', 'a', 'guy', 'who', 'wanted', 'some', 'crabs', 'and', 'a', 'goldfish', 'i', 'found', 'a', 'guy', 'selling', 'some', 'crabs', 'and', 'a', 'goldfish', 'but', 'the', 'original', 'guy', 'already', 'got', 'his', 'crabs', 'and', 'a', 'goldfish', 'i', 'asked', 'the', 'seller', 'for', 'a', 'refund', 'but', 'he', 'refused', 'so', 'i', 'put', 'the', 'crabs', 'and', 'the', 'goldfish', 'in', 'my', 'cloth', 'backpack', 'with', 'the', 'task', 'of', 'getting', 'rid', 'of', 'them', 'i', 'found', 'some', 'construction', 'workers', 'to', 'take', 'care', 'of', 'the', 'goldfish', 'but', 'i', 'was', 'stuck', 'with', 'the', 'crabs', 'that', 'were', 'crawling', 'around', 'in', 'my', 'bag', 'i', 'somehow', 'teleported', 'to', 'a', 'japanese', 'airport', 'and', 'i', 'went', 'into', 'the', 'bathroom', 'when', 'i', 'felt', 'a', 'poke', 'in', 'my', 'back', 'i', 'threw', 'my', 'bag', 'and', 'i', 'saw', 'claws', 'ing', 'out', 'of', 'the', 'bag', 'i', 'threw', 'my', 'bag', 'on', 'the', 'floor', 'a', 'woman', 'opened', 'her', 'stall', 'and', 'fainted', 'and', 'i', 'was', 'like', '“you', 'live', 'on', 'an', 'island', 'why', 'are', 'you', 'scared', 'of', 'crabs', '”', 'a', 'janitor', 'in', 'the', 'back', 'told', 'me', 'to', 'e', 'with', 'him', 'but', 'before', 'we', 'got', 'rid', 'of', 'the', 'crabs', 'i', 'woke', 'up', 'my', 'question', 'is', 'if', 'i', 'place', 'a', 'crab', 'in', 'a', 'box', 'made', 'out', 'of', 'cloth', 'will', 'it', 'be', 'smart', 'enough', 'to', 'try', 'and', 'tear', 'through', 'it', 'the', 'crabs', 'in', 'my', 'dream', 'were', 'baby', 'crabs', 'if', 'that', 'matters'], tags=['Train_8']),\n",
       " TaggedDocument(words=['when', 'i', 'touch', 'my', 'cat', 'sometimes'], tags=['Train_9'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87990/87990 [00:00<00:00, 3136451.25it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3681878.86it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3240182.69it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3352654.08it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3509378.86it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3345117.78it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3464573.93it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3671147.72it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3352532.26it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3235382.17it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3497306.91it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3522542.80it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3211507.51it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3351618.87it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3263967.53it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3365986.05it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 2967243.21it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3385904.41it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3350979.80it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3569870.76it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3617140.14it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 2996028.71it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3530630.53it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3682613.64it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3266018.36it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3602768.62it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3426806.77it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3588964.50it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3633056.80it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 3511348.84it/s]\n",
      "100%|██████████| 87990/87990 [00:00<00:00, 2970323.94it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=2, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = data.name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 444 ms, sys: 118 ms, total: 562 ms\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=10, solver='lbfgs', multi_class='multinomial')\n",
    "logreg = logreg.fit(train_vectors_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6345796870856537\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       gameofthrones       0.89      0.92      0.90       733\n",
      "           teenagers       0.35      0.16      0.22       755\n",
      "           AskReddit       0.80      0.92      0.86      5579\n",
      "                pics       0.57      0.51      0.54      1494\n",
      "           dankmemes       0.60      0.59      0.60      1679\n",
      "RocketLeagueExchange       0.99      0.99      0.99       999\n",
      "                 aww       0.49      0.42      0.45       889\n",
      "              me_irl       0.50      0.61      0.55      1214\n",
      "               funny       0.26      0.11      0.15       771\n",
      "      Showerthoughts       0.49      0.49      0.49       958\n",
      "               memes       0.25      0.34      0.29      1448\n",
      "    unpopularopinion       0.73      0.76      0.74       910\n",
      "       AmItheAsshole       0.51      0.52      0.51      1253\n",
      "              videos       0.81      0.95      0.87      1031\n",
      "            freefolk       0.24      0.05      0.08       678\n",
      "              gaming       0.48      0.48      0.48      1345\n",
      "               Jokes       0.84      0.88      0.86      1688\n",
      "   NoStupidQuestions       0.49      0.45      0.47      1030\n",
      "            AskOuija       0.72      0.68      0.70       589\n",
      "            politics       0.45      0.40      0.42      1354\n",
      "\n",
      "         avg / total       0.61      0.63      0.62     26397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=subreddits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User input test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me_irl']\n"
     ]
    }
   ],
   "source": [
    "input_title = \"no spoilers  how does rising the dead work\"\n",
    "input_text = \"some stuff i wonder about how fresh do the dead need to be  can the nk rise people ho died weeks months years ago and how close\"\n",
    "all_input = [input_title + ' ' + input_text]\n",
    "user_input = model_dbow.infer_vector(all_input, steps=30)\n",
    "user_input = user_input.reshape(1, -1)\n",
    "prediction = logreg.predict(user_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ivd = {v: k for k, v in y_encode.items()}\n",
    "# [ivd[x] for x in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.nlargest of                                  0\n",
       "AmItheAsshole         9.852817e-06\n",
       "AskOuija              2.234648e-03\n",
       "AskReddit             7.556596e-06\n",
       "Jokes                 1.777611e-06\n",
       "NoStupidQuestions     4.047150e-08\n",
       "RocketLeagueExchange  2.227789e-07\n",
       "Showerthoughts        7.535379e-06\n",
       "aww                   1.945973e-02\n",
       "dankmemes             2.664732e-02\n",
       "freefolk              4.265949e-04\n",
       "funny                 2.276930e-02\n",
       "gameofthrones         7.659893e-07\n",
       "gaming                7.855380e-04\n",
       "me_irl                9.007786e-01\n",
       "memes                 2.259229e-02\n",
       "pics                  3.709937e-03\n",
       "politics              1.645872e-05\n",
       "teenagers             5.415296e-06\n",
       "unpopularopinion      7.220588e-12\n",
       "videos                5.464293e-04>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_df = \n",
    "pd.DataFrame(logreg.predict_proba(user_input), columns=logreg.classes_).T.nlargest\n",
    "# top_5 = output_df.nlargest(5, [0])\n",
    "# top_5.reset_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(logreg.predict_proba(user_input), columns=logreg.classes_).T.nlargest(5, [0]).reset_index().values\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
